{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to tips for CP4D and OpenShift Here are some docs on tips on installing and managing CP4D and OpenShift. If you have any questions, comments or issues, please send me an email ronan.bourlier@fr.ibm.com Documents Getting external access to Db2 database installed in CP4D on a ROKS VPC Gen2 OpenShift client installation and tips Clean Db2 logs in pods CP4D versions in GUI HAProxy configuration for DV Db2 access","title":"Home"},{"location":"#welcome-to-tips-for-cp4d-and-openshift","text":"Here are some docs on tips on installing and managing CP4D and OpenShift. If you have any questions, comments or issues, please send me an email ronan.bourlier@fr.ibm.com","title":"Welcome to tips for CP4D and OpenShift"},{"location":"#documents","text":"Getting external access to Db2 database installed in CP4D on a ROKS VPC Gen2 OpenShift client installation and tips Clean Db2 logs in pods CP4D versions in GUI HAProxy configuration for DV Db2 access","title":"Documents"},{"location":"cp4d-version/","text":"How to get CP4D version Here is a quick tip to get the CP4D versions in your browser. Add zen/#/versionDetails at the end of the URL. For instance https://cpd-dfi.apps.ocp.nca.ihost.com/zen/#/versionDetails Here is also the JSON file you can get. { \"Control plane\" : { \"version\" : \"4.4.2\" }, \"dmc\" : { \"version\" : \"4.0.7\" }, \"dg\" : { \"version\" : \"2.0.7\" }, \"Common Core Services\" : { \"version\" : \"4.0.7\" }, \"Watson Knowledge Catalog\" : { \"version\" : \"4.0.7\" }, \"IBM Runtime 22.1 on Python 3.9\" : { \"version\" : \"4.0.7\" }, \"Jupyter Notebook Server with Python 3.8\" : { \"version\" : \"4.0.7\" }, \"IBM Decision Optimization\" : { \"version\" : \"4.0.7 (4.0.7+b399)\" } } The global CP4D version is mentionned in Common Core Services","title":"CP4D versions"},{"location":"cp4d-version/#how-to-get-cp4d-version","text":"Here is a quick tip to get the CP4D versions in your browser. Add zen/#/versionDetails at the end of the URL. For instance https://cpd-dfi.apps.ocp.nca.ihost.com/zen/#/versionDetails Here is also the JSON file you can get. { \"Control plane\" : { \"version\" : \"4.4.2\" }, \"dmc\" : { \"version\" : \"4.0.7\" }, \"dg\" : { \"version\" : \"2.0.7\" }, \"Common Core Services\" : { \"version\" : \"4.0.7\" }, \"Watson Knowledge Catalog\" : { \"version\" : \"4.0.7\" }, \"IBM Runtime 22.1 on Python 3.9\" : { \"version\" : \"4.0.7\" }, \"Jupyter Notebook Server with Python 3.8\" : { \"version\" : \"4.0.7\" }, \"IBM Decision Optimization\" : { \"version\" : \"4.0.7 (4.0.7+b399)\" } } The global CP4D version is mentionned in Common Core Services","title":"How to get CP4D version"},{"location":"db2-prune-logs/","text":"Prune Db2 databases logs to avoid out of space issue By default, db2 will save logs and there is no log rotation configured by default. This can lead to out of space issue in the PVC (ODF) or worst in the whole storage (NFS) Here is the procedure. Log in to Openshift environment and go to your CP4D project. $ oc login -u apikey -p XXXXXXXX --server=https://c100-e.eu-de.containers.cloud.ibm.com:NNNN Login successful. You have access to 69 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"cp4d\". $ oc project cp4d Already on project \"cp4d\" on server \"https://c100-e.eu-de.containers.cloud.ibm.com:NNNNN\" In CP4D 4.0.3+, let's turn auto prune archieve_log oc rsh c-db2u-dv-db2u-0 bash su - db2inst1 bigsql_version = $( cat /opt/dv/current/version | cut -d '=' -f2 ) /usr/ibmpacks/bigsql/ ${ bigsql_version } /bigsql/bigsql-cli/BIGSQL/package/scripts/bigsql-auto-logprune.sh -U db2inst1 -M Enable In CP4D 4.0.2-, let's install a script to remove old db2 logs Add the following script to a file, say dv-db2-log-cleanup.sh, then run it at the backgroup while true ; do #We need to restrict the file type otherwise find cmd may find Gaiandb db files that must be kept in the pod #We keep all the db2diag*.log as they currently have a DIAGSIZE of 300MB find ${ DIAGPATH } -atime +1 -type f -name \"*.log\" ! -name \"db2diag*.log\" -exec ls -l {} \\; find ${ DIAGPATH } -atime +1 -type f -name \"*.log\" ! -name \"db2diag*.log\" -exec rm -f {} \\; find ${ DIAGPATH } -atime +1 -type f -name \"*.txt\" -exec ls -l {} \\; find ${ DIAGPATH } -atime +1 -type f -name \"*.txt\" -exec rm -f {} \\; find ${ DIAGPATH } -atime +1 -type d -name \"FODC*\" -exec ls -d {} \\; find ${ DIAGPATH } -atime +1 -type d -name \"FODC*\" -exec rm -rf {} \\; find ${ DIAGPATH } -atime +1 -type f -name \"core*\" -exec ls -l {} \\; find ${ DIAGPATH } -atime +1 -type f -name \"core*\" -exec rm -f {} \\; sleep 24h #run once every 24 hours done oc rsh c-db2u-dv-db2u-0 bash su - db2inst1 vi dv-db2-log-cleanup.sh chmod +x dv-db2-log-cleanup.sh ; nohup ./dv-db2-log-cleanup.sh & This script can also be used to clean a db2 installation with a full disk, if you can still log in to it long enough.","title":"Prune Db2 logs"},{"location":"db2-prune-logs/#prune-db2-databases-logs-to-avoid-out-of-space-issue","text":"By default, db2 will save logs and there is no log rotation configured by default. This can lead to out of space issue in the PVC (ODF) or worst in the whole storage (NFS) Here is the procedure. Log in to Openshift environment and go to your CP4D project. $ oc login -u apikey -p XXXXXXXX --server=https://c100-e.eu-de.containers.cloud.ibm.com:NNNN Login successful. You have access to 69 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"cp4d\". $ oc project cp4d Already on project \"cp4d\" on server \"https://c100-e.eu-de.containers.cloud.ibm.com:NNNNN\"","title":"Prune Db2 databases logs to avoid out of space issue"},{"location":"db2-prune-logs/#in-cp4d-403-lets-turn-auto-prune-archieve_log","text":"oc rsh c-db2u-dv-db2u-0 bash su - db2inst1 bigsql_version = $( cat /opt/dv/current/version | cut -d '=' -f2 ) /usr/ibmpacks/bigsql/ ${ bigsql_version } /bigsql/bigsql-cli/BIGSQL/package/scripts/bigsql-auto-logprune.sh -U db2inst1 -M Enable","title":"In CP4D 4.0.3+, let's turn auto prune archieve_log"},{"location":"db2-prune-logs/#in-cp4d-402-lets-install-a-script-to-remove-old-db2-logs","text":"Add the following script to a file, say dv-db2-log-cleanup.sh, then run it at the backgroup while true ; do #We need to restrict the file type otherwise find cmd may find Gaiandb db files that must be kept in the pod #We keep all the db2diag*.log as they currently have a DIAGSIZE of 300MB find ${ DIAGPATH } -atime +1 -type f -name \"*.log\" ! -name \"db2diag*.log\" -exec ls -l {} \\; find ${ DIAGPATH } -atime +1 -type f -name \"*.log\" ! -name \"db2diag*.log\" -exec rm -f {} \\; find ${ DIAGPATH } -atime +1 -type f -name \"*.txt\" -exec ls -l {} \\; find ${ DIAGPATH } -atime +1 -type f -name \"*.txt\" -exec rm -f {} \\; find ${ DIAGPATH } -atime +1 -type d -name \"FODC*\" -exec ls -d {} \\; find ${ DIAGPATH } -atime +1 -type d -name \"FODC*\" -exec rm -rf {} \\; find ${ DIAGPATH } -atime +1 -type f -name \"core*\" -exec ls -l {} \\; find ${ DIAGPATH } -atime +1 -type f -name \"core*\" -exec rm -f {} \\; sleep 24h #run once every 24 hours done oc rsh c-db2u-dv-db2u-0 bash su - db2inst1 vi dv-db2-log-cleanup.sh chmod +x dv-db2-log-cleanup.sh ; nohup ./dv-db2-log-cleanup.sh & This script can also be used to clean a db2 installation with a full disk, if you can still log in to it long enough.","title":"In CP4D 4.0.2-, let's install a script to remove old db2 logs"},{"location":"db2-roks/","text":"View Db2 databases externally from a CP4D installed on ROKS VPC Gen2 A specificity from ROKS VPC Gen2 in IBM Cloud is the security level. The node servers don't have an external IP address and cannot be accessed externally. Once you have installed Db2 in CP4D, the usual way to vie the database externally is by using NodePort access. In our case this is not working since the node cannot be viewed externally. The solution is to use a Load Balancer that needs to be added. Here is the procedure. Log in to Openshift environment and go to your CP4D project. $ oc login -u apikey -p XXXXXXXX --server=https://c100-e.eu-de.containers.cloud.ibm.com:NNNN Login successful. You have access to 69 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"cp4d\". $ oc project cp4d Already on project \"cp4d\" on server \"https://c100-e.eu-de.containers.cloud.ibm.com:NNNNN\" Find the pods for your database. In the Databases > Details page, find the Deployment id. Note down the deployment id as it it will used all along the procedure. In this case db2oltp-1638208426839597 Because a VPC in IBM Cloud is oriented toward security, workers nodes are not visible from outside of its LAN. We cannot use a regular NodePort service. Let's use a LoadBalancer service in this case. Here is the example file apiVersion : v1 kind : Service metadata : name : lb-db2-2 annotations : service.kubernetes.io/ibm-load-balancer-cloud-provider-ip-type : \"public\" spec : ports : - name : db protocol : TCP port : 51000 targetPort : 50000 - name : db-ssl protocol : TCP port : 51001 targetPort : 50001 type : LoadBalancer selector : app : db2oltp-1638208426839597 component : db2oltp formation_id : db2oltp-1638208426839597 role : db type : engine Some explanations. - metadata.name : use the name you want. I personaly use lb- followed by the name of my database - ports : ports name are the one you want. I keep it to db and db-ssl to know what it is. I this example I create 2 ports in my load balancer forwarding to internal database port. The external port 51000 forwards to the non SSL 50000 database port. the external port 51001 forwards to the SSL 50001 database port. - selector: app and formation_id point to the deployment id we noted down earlier. Once your file configured and saved as lb-db2.yaml , let's use it. $ oc create -f db2-lb.yaml service/lb-db2-2 created $ oc get svc lb-db2-2 NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE lb-db2-2 LoadBalancer 172.21.100.200 <pending> 51000:32149/TCP,51001:32514/TCP 17s This command will trigger the creation of a Load Balancer in VPC Once the load balancer is created, you get all information in the command line. $ oc get svc lb-db2-2 NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE lb-db2-2 LoadBalancer 172.21.100.200 fbec480d-eu-de.lb.appdomain.cloud 51000:32149/TCP,51001:32514/TCP 21m With this command you get the domain name to connect to your database. In our case fbec480d-eu-de.lb.appdomain.cloud and you can test it. $ nc -zv fbec480d-eu-de.lb.appdomain.cloud 51000 Connection to fbec480d-eu-de.lb.appdomain.cloud (158.177.15.62) 51000 port [tcp/*] succeeded! $ nc -zv fbec480d-eu-de.lb.appdomain.cloud 51001 Connection to fbec480d-eu-de.lb.appdomain.cloud (158.177.15.62) 51001 port [tcp/*] succeeded! This is the best solution for now and I am still trying to find an easier solution since this one needs to be done for each database created that need an external access. Connexion from a IDE Here is an example of a connexion through an IDE. I use DBeaver. More details If you need more details on the load balancer, you can find it in the VPC load balancer DV connexion You can use the exact same method to access Data Virtualisation database externally Here is an example file apiVersion : v1 kind : Service metadata : name : lb-dv-2 annotations : service.kubernetes.io/ibm-load-balancer-cloud-provider-ip-type : \u201cpublic\u201d spec : ports : - name : db protocol : TCP port : 52000 targetPort : 50000 - name : db-ssl protocol : TCP port : 52001 targetPort : 50001 type : LoadBalancer selector : app : db2u-dv component : db2dv formation_id : db2u-dv role : db type : engine","title":"Db2 on ROKS Gen2"},{"location":"db2-roks/#view-db2-databases-externally-from-a-cp4d-installed-on-roks-vpc-gen2","text":"A specificity from ROKS VPC Gen2 in IBM Cloud is the security level. The node servers don't have an external IP address and cannot be accessed externally. Once you have installed Db2 in CP4D, the usual way to vie the database externally is by using NodePort access. In our case this is not working since the node cannot be viewed externally. The solution is to use a Load Balancer that needs to be added. Here is the procedure. Log in to Openshift environment and go to your CP4D project. $ oc login -u apikey -p XXXXXXXX --server=https://c100-e.eu-de.containers.cloud.ibm.com:NNNN Login successful. You have access to 69 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"cp4d\". $ oc project cp4d Already on project \"cp4d\" on server \"https://c100-e.eu-de.containers.cloud.ibm.com:NNNNN\" Find the pods for your database. In the Databases > Details page, find the Deployment id. Note down the deployment id as it it will used all along the procedure. In this case db2oltp-1638208426839597 Because a VPC in IBM Cloud is oriented toward security, workers nodes are not visible from outside of its LAN. We cannot use a regular NodePort service. Let's use a LoadBalancer service in this case. Here is the example file apiVersion : v1 kind : Service metadata : name : lb-db2-2 annotations : service.kubernetes.io/ibm-load-balancer-cloud-provider-ip-type : \"public\" spec : ports : - name : db protocol : TCP port : 51000 targetPort : 50000 - name : db-ssl protocol : TCP port : 51001 targetPort : 50001 type : LoadBalancer selector : app : db2oltp-1638208426839597 component : db2oltp formation_id : db2oltp-1638208426839597 role : db type : engine Some explanations. - metadata.name : use the name you want. I personaly use lb- followed by the name of my database - ports : ports name are the one you want. I keep it to db and db-ssl to know what it is. I this example I create 2 ports in my load balancer forwarding to internal database port. The external port 51000 forwards to the non SSL 50000 database port. the external port 51001 forwards to the SSL 50001 database port. - selector: app and formation_id point to the deployment id we noted down earlier. Once your file configured and saved as lb-db2.yaml , let's use it. $ oc create -f db2-lb.yaml service/lb-db2-2 created $ oc get svc lb-db2-2 NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE lb-db2-2 LoadBalancer 172.21.100.200 <pending> 51000:32149/TCP,51001:32514/TCP 17s This command will trigger the creation of a Load Balancer in VPC Once the load balancer is created, you get all information in the command line. $ oc get svc lb-db2-2 NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE lb-db2-2 LoadBalancer 172.21.100.200 fbec480d-eu-de.lb.appdomain.cloud 51000:32149/TCP,51001:32514/TCP 21m With this command you get the domain name to connect to your database. In our case fbec480d-eu-de.lb.appdomain.cloud and you can test it. $ nc -zv fbec480d-eu-de.lb.appdomain.cloud 51000 Connection to fbec480d-eu-de.lb.appdomain.cloud (158.177.15.62) 51000 port [tcp/*] succeeded! $ nc -zv fbec480d-eu-de.lb.appdomain.cloud 51001 Connection to fbec480d-eu-de.lb.appdomain.cloud (158.177.15.62) 51001 port [tcp/*] succeeded! This is the best solution for now and I am still trying to find an easier solution since this one needs to be done for each database created that need an external access.","title":"View Db2 databases externally from a CP4D installed on ROKS VPC Gen2"},{"location":"db2-roks/#connexion-from-a-ide","text":"Here is an example of a connexion through an IDE. I use DBeaver.","title":"Connexion from a IDE"},{"location":"db2-roks/#more-details","text":"If you need more details on the load balancer, you can find it in the VPC load balancer","title":"More details"},{"location":"db2-roks/#dv-connexion","text":"You can use the exact same method to access Data Virtualisation database externally Here is an example file apiVersion : v1 kind : Service metadata : name : lb-dv-2 annotations : service.kubernetes.io/ibm-load-balancer-cloud-provider-ip-type : \u201cpublic\u201d spec : ports : - name : db protocol : TCP port : 52000 targetPort : 50000 - name : db-ssl protocol : TCP port : 52001 targetPort : 50001 type : LoadBalancer selector : app : db2u-dv component : db2dv formation_id : db2u-dv role : db type : engine","title":"DV connexion"},{"location":"dv-access-configuration-haproxy/","text":"DV Access configuration on HAProxy sudo vi /etc/haproxy/haproxy.cfg Add the following lines at the end frontend dv-db2 bind *:31599 default_backend dv-db2 option tcplog backend dv-db2 balance source server worker1-http-router0 10 .3.67.21:31599 check server worker2-http-router1 10 .3.67.22:31599 check server worker3-http-router2 10 .3.67.23:31599 check server worker4-http-router3 10 .3.67.24:31599 check server worker5-http-router4 10 .3.67.25:31599 check Restart and check HAProxy $ sudo systemctl restart haproxy $ sudo systemctl status haproxy \u25cf haproxy.service - HAProxy Load Balancer Loaded: loaded ( /usr/lib/systemd/system/haproxy.service ; enabled ; vendor preset: disabled ) Drop-In: /etc/systemd/system/haproxy.service.d \u2514\u2500restart.conf Active: active ( running ) since Mon 2023 -01-16 03 :36:07 EST ; 5s ago Process: 2310709 ExecStartPre = /usr/sbin/haproxy -f $CONFIG -f $CFGDIR -c -q $OPTIONS ( code = exited, status = 0 /SUCCESS ) Main PID: 2310712 ( haproxy ) Tasks: 2 ( limit: 75278 ) Memory: 10 .3M CGroup: /system.slice/haproxy.service \u251c\u25002310712 /usr/sbin/haproxy -Ws -f /etc/haproxy/haproxy.cfg -f /etc/haproxy/conf.d -p /run/haproxy.pid \u2514\u25002310716 /usr/sbin/haproxy -Ws -f /etc/haproxy/haproxy.cfg -f /etc/haproxy/conf.d -p /run/haproxy.pid Jan 16 03 :36:07 bastion.cp4d.techsales.mop.ibm haproxy [ 2310712 ] : [ WARNING ] 015 /033607 ( 2310712 ) : config : 'option forwardfor' ignored for backend 'openshift-api-server' as it requires HTTP mode. Jan 16 03 :36:07 bastion.cp4d.techsales.mop.ibm haproxy [ 2310712 ] : [ WARNING ] 015 /033607 ( 2310712 ) : config : 'option forwardfor' ignored for frontend 'machine-config-server' as it requires HTTP mode. Jan 16 03 :36:07 bastion.cp4d.techsales.mop.ibm haproxy [ 2310712 ] : [ WARNING ] 015 /033607 ( 2310712 ) : config : 'option forwardfor' ignored for backend 'machine-config-server' as it requires HTTP mode. Jan 16 03 :36:07 bastion.cp4d.techsales.mop.ibm haproxy [ 2310712 ] : [ WARNING ] 015 /033607 ( 2310712 ) : config : 'option forwardfor' ignored for frontend 'ingress-http' as it requires HTTP mode. Jan 16 03 :36:07 bastion.cp4d.techsales.mop.ibm haproxy [ 2310712 ] : [ WARNING ] 015 /033607 ( 2310712 ) : config : 'option forwardfor' ignored for backend 'ingress-http' as it requires HTTP mode. Jan 16 03 :36:07 bastion.cp4d.techsales.mop.ibm haproxy [ 2310712 ] : [ WARNING ] 015 /033607 ( 2310712 ) : config : 'option forwardfor' ignored for frontend 'ingress-https' as it requires HTTP mode. Jan 16 03 :36:07 bastion.cp4d.techsales.mop.ibm haproxy [ 2310712 ] : [ WARNING ] 015 /033607 ( 2310712 ) : config : 'option forwardfor' ignored for backend 'ingress-https' as it requires HTTP mode. Jan 16 03 :36:07 bastion.cp4d.techsales.mop.ibm haproxy [ 2310712 ] : [ WARNING ] 015 /033607 ( 2310712 ) : config : 'option forwardfor' ignored for frontend 'dv-db2' as it requires HTTP mode. Jan 16 03 :36:07 bastion.cp4d.techsales.mop.ibm haproxy [ 2310712 ] : [ WARNING ] 015 /033607 ( 2310712 ) : config : 'option forwardfor' ignored for backend 'dv-db2' as it requires HTTP mode. Jan 16 03 :36:07 bastion.cp4d.techsales.mop.ibm systemd [ 1 ] : Started HAProxy Load Balancer. Open Firewall sudo firewall-cmd --list-ports sudo firewall-cmd --add-port = 31599 /tcp --permanent sudo firewall-cmd --reload sudo firewall-cmd --list-ports Test connexion $ nc -zv cpd-cpd452.apps.cp4d.techsales.mop.ibm 31599 Ncat: Version 7 .70 ( https://nmap.org/ncat ) Ncat: Connected to 10 .3.67.1:3159. Ncat: 0 bytes sent, 0 bytes received in 0 .01 seconds. DBeaver configuration Don\u2019t set up the securityMechanism parameter as it didn\u2019t work for me. With just this configuration it works. Check Db2 configuration db2set -all Check DB2COMM includes TCPIP","title":"HAProxy configuration for DV Db2 access"},{"location":"dv-access-configuration-haproxy/#dv-access-configuration-on-haproxy","text":"sudo vi /etc/haproxy/haproxy.cfg Add the following lines at the end frontend dv-db2 bind *:31599 default_backend dv-db2 option tcplog backend dv-db2 balance source server worker1-http-router0 10 .3.67.21:31599 check server worker2-http-router1 10 .3.67.22:31599 check server worker3-http-router2 10 .3.67.23:31599 check server worker4-http-router3 10 .3.67.24:31599 check server worker5-http-router4 10 .3.67.25:31599 check Restart and check HAProxy $ sudo systemctl restart haproxy $ sudo systemctl status haproxy \u25cf haproxy.service - HAProxy Load Balancer Loaded: loaded ( /usr/lib/systemd/system/haproxy.service ; enabled ; vendor preset: disabled ) Drop-In: /etc/systemd/system/haproxy.service.d \u2514\u2500restart.conf Active: active ( running ) since Mon 2023 -01-16 03 :36:07 EST ; 5s ago Process: 2310709 ExecStartPre = /usr/sbin/haproxy -f $CONFIG -f $CFGDIR -c -q $OPTIONS ( code = exited, status = 0 /SUCCESS ) Main PID: 2310712 ( haproxy ) Tasks: 2 ( limit: 75278 ) Memory: 10 .3M CGroup: /system.slice/haproxy.service \u251c\u25002310712 /usr/sbin/haproxy -Ws -f /etc/haproxy/haproxy.cfg -f /etc/haproxy/conf.d -p /run/haproxy.pid \u2514\u25002310716 /usr/sbin/haproxy -Ws -f /etc/haproxy/haproxy.cfg -f /etc/haproxy/conf.d -p /run/haproxy.pid Jan 16 03 :36:07 bastion.cp4d.techsales.mop.ibm haproxy [ 2310712 ] : [ WARNING ] 015 /033607 ( 2310712 ) : config : 'option forwardfor' ignored for backend 'openshift-api-server' as it requires HTTP mode. Jan 16 03 :36:07 bastion.cp4d.techsales.mop.ibm haproxy [ 2310712 ] : [ WARNING ] 015 /033607 ( 2310712 ) : config : 'option forwardfor' ignored for frontend 'machine-config-server' as it requires HTTP mode. Jan 16 03 :36:07 bastion.cp4d.techsales.mop.ibm haproxy [ 2310712 ] : [ WARNING ] 015 /033607 ( 2310712 ) : config : 'option forwardfor' ignored for backend 'machine-config-server' as it requires HTTP mode. Jan 16 03 :36:07 bastion.cp4d.techsales.mop.ibm haproxy [ 2310712 ] : [ WARNING ] 015 /033607 ( 2310712 ) : config : 'option forwardfor' ignored for frontend 'ingress-http' as it requires HTTP mode. Jan 16 03 :36:07 bastion.cp4d.techsales.mop.ibm haproxy [ 2310712 ] : [ WARNING ] 015 /033607 ( 2310712 ) : config : 'option forwardfor' ignored for backend 'ingress-http' as it requires HTTP mode. Jan 16 03 :36:07 bastion.cp4d.techsales.mop.ibm haproxy [ 2310712 ] : [ WARNING ] 015 /033607 ( 2310712 ) : config : 'option forwardfor' ignored for frontend 'ingress-https' as it requires HTTP mode. Jan 16 03 :36:07 bastion.cp4d.techsales.mop.ibm haproxy [ 2310712 ] : [ WARNING ] 015 /033607 ( 2310712 ) : config : 'option forwardfor' ignored for backend 'ingress-https' as it requires HTTP mode. Jan 16 03 :36:07 bastion.cp4d.techsales.mop.ibm haproxy [ 2310712 ] : [ WARNING ] 015 /033607 ( 2310712 ) : config : 'option forwardfor' ignored for frontend 'dv-db2' as it requires HTTP mode. Jan 16 03 :36:07 bastion.cp4d.techsales.mop.ibm haproxy [ 2310712 ] : [ WARNING ] 015 /033607 ( 2310712 ) : config : 'option forwardfor' ignored for backend 'dv-db2' as it requires HTTP mode. Jan 16 03 :36:07 bastion.cp4d.techsales.mop.ibm systemd [ 1 ] : Started HAProxy Load Balancer. Open Firewall sudo firewall-cmd --list-ports sudo firewall-cmd --add-port = 31599 /tcp --permanent sudo firewall-cmd --reload sudo firewall-cmd --list-ports Test connexion $ nc -zv cpd-cpd452.apps.cp4d.techsales.mop.ibm 31599 Ncat: Version 7 .70 ( https://nmap.org/ncat ) Ncat: Connected to 10 .3.67.1:3159. Ncat: 0 bytes sent, 0 bytes received in 0 .01 seconds.","title":"DV Access configuration on HAProxy"},{"location":"dv-access-configuration-haproxy/#dbeaver-configuration","text":"Don\u2019t set up the securityMechanism parameter as it didn\u2019t work for me. With just this configuration it works.","title":"DBeaver configuration"},{"location":"dv-access-configuration-haproxy/#check-db2-configuration","text":"db2set -all Check DB2COMM includes TCPIP","title":"Check Db2 configuration"},{"location":"oc-client-install/","text":"OpenShift client installation en tips Here are the steps to install the OpenShift client for Linux and some tips ans tricks to manage connexions Install oc client Navigate to the OpenShift Container Platform downloads page on the Red Hat Customer Portal. Select the appropriate version in the Version drop-down menu. Right Click on \"Download Now next to the latest OpenShift Linux Client entry to get the link. Download the client with the link and wget. Don't forget the single quote around the URL to avoind issues. wget 'https://access.cdn.redhat.com/content/origin/files/sha256/28/2895de3bb4a9d9a68aa6e48c06ffaed21f81c7341a78e81fddd7c50eaca08c1b/oc-4.9.21-linux.tar.gz?user=c1223590990cfd8b164b125451ffffff&_auth_=1645565566_0d7edf2fef35f7623fa3dd411c310666' Rename the file for easier management mv oc-4.9.21-linux.tar.gz \\? user \\= c1223590990cfd8b164b125451ffffff \\& _auth_ \\= 1645565566_0d7edf2fef35f7623fa3dd411c310666 oc-4.9.21-linux.tar.gz (NB : I changed the content to avoid giving up my user id and auth) Unzip the file tar xvzf oc-4.9.21-linux.tar.gz Place the oc binary in a directory that is on your PATH. sudo mv oc kubectl /usr/local/bin Check that everything is working $ oc version Client Version: 4 .9.21 error: You must be logged in to the server ( Unauthorized ) The error appears if you are not logged in so don't worry. Source : Red Hat OpenShift documentation bash completion TBD How to log to ROKS OpenShift from CLI with an API key ROKS doesn't allow using oc login with username and password to log in in OpenShift. Here is the solution to get a oc login that works all the time without having to go through the Web interface every day. Install ibmcloud command curl -fsSL https://clis.cloud.ibm.com/install/linux | sh ibmcloud plugin install container-service ibmcloud plugin install container-registry ibmcloud plugin install observe-service Login to your IBM Cloud $ ibmcloud login -sso API endpoint: https://cloud.ibm.com Region: eu-de Get a one-time code from https://identity-2.uk-south.iam.cloud.ibm.com/identity/passcode to proceed. Open the URL in the default browser? [ Y/n ] > n One-time code > Authenticating... OK Select an account: 1 . Ronan Bourlier 's Account (b02822e1d9cb5263f3d06ffffdcd9793) <-> 1687099 2. WATSON CMCIC (e443898b246db06ab7af7a5ef0498e89) [...] Enter a number> 1 Targeted account Ronan Bourlier' s Account ( b02822e1d9cb5263f3d06ffffdcd9793 ) <-> 1687099 API endpoint: https://cloud.ibm.com Region: eu-de User: ronan.bourlier@fr.ibm.com Account: Ronan Bourlier 's Account (b02822e1d9cb5263f3d06ffffdcd9793) <-> 1687099 Resource group: No resource group targeted, use ' ibmcloud target -g RESOURCE_GROUP ' CF API endpoint: Org: Space: List your clusters and get the ID your are interested in $ ibmcloud oc cluster ls OK Name ID State Created Workers Location Version Resource Group Name Provider prod01 c7sh01kf0sdsn628u6cg normal 3 weeks ago 11 Frankfurt 4 .8.26_1542_openshift default vpc-gen2 prod02 c8ab34rf0s84mif4ffeg normal 10 hours ago 11 Frankfurt 4 .8.26_1542_openshift default vpc-gen2 test01 c80pi0mf05u00m1elfgg normal 2 weeks ago 11 Frankfurt 4 .8.26_1542_openshift default vpc-gen2 test03 c69spr6f0iqs72cnn5jg normal 3 months ago 8 Frankfurt 4 .8.26_1542_openshift default vpc-gen2 test04 c7il1kkf09bln89tt09g normal 1 month ago 11 Frankfurt 4 .8.26_1542_openshift default vpc-gen2 Get URL for API connect Select your cluster and get the URL export CLUSTER_NAME = prod02 export MASTER_URL = $( ibmcloud oc cluster get -c $CLUSTER_NAME | awk '/^URL:/ {print $2}' ) With latest version of kubernetes plugin export CLUSTER_NAME = prod02 export MASTER_URL = $( ibmcloud oc cluster get -c $CLUSTER_NAME | awk '/^Master URL:/ {print $3}' ) Create an API key for your environment Choose the name you want, I chose prod02-ocp in this example $ ibmcloud iam api-key-create prod02-ocp Creating API key prod02-ocp under b02822e1d9cb5263f3d06ffffdcd9793 as ronan.bourlier@fr.ibm.com... OK API key prod02-ocp was created Please preserve the API key! It cannot be retrieved after it ' s created. ID ApiKey-e30537fa-01ab-422f-afbb-68e77dab3bfb Name prod02-ocp Description Created At 2022 -02-22T21:01+0000 API Key 0h6teIifwifCAD61qyXXXX9lQZzQ0RkkNnvYzmaGsTQ7 Locked false Keep in a safe place the API Key, you cannot retrieve it after. Of course I changed mine in this example ;) Login with this key in IBM Cloud ibmcloud login --apikey 0h6teIifwifCAD61qyXXXX9lQZzQ0RkkNnvYzmaGsTQ7 Get context for your cluster ibmcloud oc cluster config -c prod02 Now you can login with your API key oc login -u apikey -p 0h6teIifwifCAD61qyXXXX9lQZzQ0RkkNnvYzmaGsTQ7 --server = https://c100-e.eu-de.containers.cloud.ibm.com:31683 I replaced $MASTER_URL by its value to keep this login in a place to use it easily.","title":"Openshift Client"},{"location":"oc-client-install/#openshift-client-installation-en-tips","text":"Here are the steps to install the OpenShift client for Linux and some tips ans tricks to manage connexions","title":"OpenShift client installation en tips"},{"location":"oc-client-install/#install-oc-client","text":"Navigate to the OpenShift Container Platform downloads page on the Red Hat Customer Portal. Select the appropriate version in the Version drop-down menu. Right Click on \"Download Now next to the latest OpenShift Linux Client entry to get the link. Download the client with the link and wget. Don't forget the single quote around the URL to avoind issues. wget 'https://access.cdn.redhat.com/content/origin/files/sha256/28/2895de3bb4a9d9a68aa6e48c06ffaed21f81c7341a78e81fddd7c50eaca08c1b/oc-4.9.21-linux.tar.gz?user=c1223590990cfd8b164b125451ffffff&_auth_=1645565566_0d7edf2fef35f7623fa3dd411c310666' Rename the file for easier management mv oc-4.9.21-linux.tar.gz \\? user \\= c1223590990cfd8b164b125451ffffff \\& _auth_ \\= 1645565566_0d7edf2fef35f7623fa3dd411c310666 oc-4.9.21-linux.tar.gz (NB : I changed the content to avoid giving up my user id and auth) Unzip the file tar xvzf oc-4.9.21-linux.tar.gz Place the oc binary in a directory that is on your PATH. sudo mv oc kubectl /usr/local/bin Check that everything is working $ oc version Client Version: 4 .9.21 error: You must be logged in to the server ( Unauthorized ) The error appears if you are not logged in so don't worry. Source : Red Hat OpenShift documentation","title":"Install oc client"},{"location":"oc-client-install/#bash-completion","text":"TBD","title":"bash completion"},{"location":"oc-client-install/#how-to-log-to-roks-openshift-from-cli-with-an-api-key","text":"ROKS doesn't allow using oc login with username and password to log in in OpenShift. Here is the solution to get a oc login that works all the time without having to go through the Web interface every day.","title":"How to log to ROKS OpenShift from CLI with an API key"},{"location":"oc-client-install/#install-ibmcloud-command","text":"curl -fsSL https://clis.cloud.ibm.com/install/linux | sh ibmcloud plugin install container-service ibmcloud plugin install container-registry ibmcloud plugin install observe-service","title":"Install ibmcloud command"},{"location":"oc-client-install/#login-to-your-ibm-cloud","text":"$ ibmcloud login -sso API endpoint: https://cloud.ibm.com Region: eu-de Get a one-time code from https://identity-2.uk-south.iam.cloud.ibm.com/identity/passcode to proceed. Open the URL in the default browser? [ Y/n ] > n One-time code > Authenticating... OK Select an account: 1 . Ronan Bourlier 's Account (b02822e1d9cb5263f3d06ffffdcd9793) <-> 1687099 2. WATSON CMCIC (e443898b246db06ab7af7a5ef0498e89) [...] Enter a number> 1 Targeted account Ronan Bourlier' s Account ( b02822e1d9cb5263f3d06ffffdcd9793 ) <-> 1687099 API endpoint: https://cloud.ibm.com Region: eu-de User: ronan.bourlier@fr.ibm.com Account: Ronan Bourlier 's Account (b02822e1d9cb5263f3d06ffffdcd9793) <-> 1687099 Resource group: No resource group targeted, use ' ibmcloud target -g RESOURCE_GROUP ' CF API endpoint: Org: Space: List your clusters and get the ID your are interested in $ ibmcloud oc cluster ls OK Name ID State Created Workers Location Version Resource Group Name Provider prod01 c7sh01kf0sdsn628u6cg normal 3 weeks ago 11 Frankfurt 4 .8.26_1542_openshift default vpc-gen2 prod02 c8ab34rf0s84mif4ffeg normal 10 hours ago 11 Frankfurt 4 .8.26_1542_openshift default vpc-gen2 test01 c80pi0mf05u00m1elfgg normal 2 weeks ago 11 Frankfurt 4 .8.26_1542_openshift default vpc-gen2 test03 c69spr6f0iqs72cnn5jg normal 3 months ago 8 Frankfurt 4 .8.26_1542_openshift default vpc-gen2 test04 c7il1kkf09bln89tt09g normal 1 month ago 11 Frankfurt 4 .8.26_1542_openshift default vpc-gen2 Get URL for API connect Select your cluster and get the URL export CLUSTER_NAME = prod02 export MASTER_URL = $( ibmcloud oc cluster get -c $CLUSTER_NAME | awk '/^URL:/ {print $2}' ) With latest version of kubernetes plugin export CLUSTER_NAME = prod02 export MASTER_URL = $( ibmcloud oc cluster get -c $CLUSTER_NAME | awk '/^Master URL:/ {print $3}' ) Create an API key for your environment Choose the name you want, I chose prod02-ocp in this example $ ibmcloud iam api-key-create prod02-ocp Creating API key prod02-ocp under b02822e1d9cb5263f3d06ffffdcd9793 as ronan.bourlier@fr.ibm.com... OK API key prod02-ocp was created Please preserve the API key! It cannot be retrieved after it ' s created. ID ApiKey-e30537fa-01ab-422f-afbb-68e77dab3bfb Name prod02-ocp Description Created At 2022 -02-22T21:01+0000 API Key 0h6teIifwifCAD61qyXXXX9lQZzQ0RkkNnvYzmaGsTQ7 Locked false Keep in a safe place the API Key, you cannot retrieve it after. Of course I changed mine in this example ;) Login with this key in IBM Cloud ibmcloud login --apikey 0h6teIifwifCAD61qyXXXX9lQZzQ0RkkNnvYzmaGsTQ7 Get context for your cluster ibmcloud oc cluster config -c prod02 Now you can login with your API key oc login -u apikey -p 0h6teIifwifCAD61qyXXXX9lQZzQ0RkkNnvYzmaGsTQ7 --server = https://c100-e.eu-de.containers.cloud.ibm.com:31683 I replaced $MASTER_URL by its value to keep this login in a place to use it easily.","title":"Login to your IBM Cloud"}]}